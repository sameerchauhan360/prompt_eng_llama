{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T05:31:44.867677Z","iopub.execute_input":"2024-09-30T05:31:44.868038Z","iopub.status.idle":"2024-09-30T05:31:45.855855Z","shell.execute_reply.started":"2024-09-30T05:31:44.868004Z","shell.execute_reply":"2024-09-30T05:31:45.854940Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# GPU llama-cpp-python\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_MAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir\n!pip install huggingface_hub\n!pip install llama-cpp-python==0.1.78\n!pip install numpy==1.23.4","metadata":{"execution":{"iopub.status.busy":"2024-09-30T05:38:53.781110Z","iopub.execute_input":"2024-09-30T05:38:53.781789Z","iopub.status.idle":"2024-09-30T05:40:28.258830Z","shell.execute_reply.started":"2024-09-30T05:38:53.781723Z","shell.execute_reply":"2024-09-30T05:40:28.257552Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python==0.1.78\n  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting numpy==1.23.4\n  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nDownloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m171.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=294850 sha256=96f7ccb1f92ac84f14998ae668d7e6ea518ddab465dd9ee43d874c05f0c0f240\n  Stored in directory: /tmp/pip-ephem-wheel-cache-w70_rw1c/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\nSuccessfully built llama-cpp-python\nInstalling collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Uninstalling typing_extensions-4.12.2:\n      Successfully uninstalled typing_extensions-4.12.2\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\nalbucore 0.0.16 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\nalbumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.4 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbayesian-optimization 1.5.1 requires numpy>=1.25, but you have numpy 1.23.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.23.4 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.4 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.4 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\nscipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\ntensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.4 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.4 which is incompatible.\nxarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\nxarray 2024.9.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.12.2\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\nRequirement already satisfied: llama-cpp-python==0.1.78 in /opt/conda/lib/python3.10/site-packages (0.1.78)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (1.23.4)\nRequirement already satisfied: diskcache>=5.6.1 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (5.6.3)\nRequirement already satisfied: numpy==1.23.4 in /opt/conda/lib/python3.10/site-packages (1.23.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name_or_path = 'TheBloke/Llama-2-13B-chat-GGML'\nmodel_basename = 'llama-2-13b-chat.ggmlv3.q5_1.bin'","metadata":{"execution":{"iopub.status.busy":"2024-09-30T05:45:39.099322Z","iopub.execute_input":"2024-09-30T05:45:39.100201Z","iopub.status.idle":"2024-09-30T05:45:39.103953Z","shell.execute_reply.started":"2024-09-30T05:45:39.100158Z","shell.execute_reply":"2024-09-30T05:45:39.103072Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download","metadata":{"execution":{"iopub.status.busy":"2024-09-30T05:44:07.768386Z","iopub.execute_input":"2024-09-30T05:44:07.769264Z","iopub.status.idle":"2024-09-30T05:44:08.138225Z","shell.execute_reply.started":"2024-09-30T05:44:07.769222Z","shell.execute_reply":"2024-09-30T05:44:08.137454Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from llama_cpp import Llama","metadata":{"execution":{"iopub.status.busy":"2024-09-30T05:44:19.096546Z","iopub.execute_input":"2024-09-30T05:44:19.097437Z","iopub.status.idle":"2024-09-30T05:44:19.101652Z","shell.execute_reply.started":"2024-09-30T05:44:19.097398Z","shell.execute_reply":"2024-09-30T05:44:19.100577Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:01:36.929114Z","iopub.execute_input":"2024-09-30T06:01:36.929978Z","iopub.status.idle":"2024-09-30T06:01:37.188101Z","shell.execute_reply.started":"2024-09-30T06:01:36.929936Z","shell.execute_reply":"2024-09-30T06:01:37.187308Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# GPU\n\nllama = Llama(\n    model_path=model_path,\n    n_batch=512,  # Adjust according to your VRAM\n    n_threads=2,  # Set number of CPU threads\n    n_gpu_layers=12  # Adjust as needed\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:08:48.513321Z","iopub.execute_input":"2024-09-30T06:08:48.513697Z","iopub.status.idle":"2024-09-30T06:08:49.090089Z","shell.execute_reply.started":"2024-09-30T06:08:48.513665Z","shell.execute_reply":"2024-09-30T06:08:49.089176Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"llama.cpp: loading model from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\nllama_model_load_internal: model size = 13B\nllama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\nllama_new_context_with_model: kv self size  =  400.00 MB\nllama_new_context_with_model: compute buffer total size =   75.35 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n","output_type":"stream"}]},{"cell_type":"code","source":"# see the n_layers in GPU\nllama.params.n_gpu_layers","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:08:50.508164Z","iopub.execute_input":"2024-09-30T06:08:50.508800Z","iopub.status.idle":"2024-09-30T06:08:50.514393Z","shell.execute_reply.started":"2024-09-30T06:08:50.508759Z","shell.execute_reply":"2024-09-30T06:08:50.513462Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"prompt = 'Write a linear regression in python'\nprompt_template = f'''SYSTEM: You are helpful, respectfull and honest assistant. Always answer as helpfull.\n\nUSER: {prompt}\n\nASSISTANT:\n'''","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:01:52.761914Z","iopub.execute_input":"2024-09-30T06:01:52.762304Z","iopub.status.idle":"2024-09-30T06:01:52.768633Z","shell.execute_reply.started":"2024-09-30T06:01:52.762268Z","shell.execute_reply":"2024-09-30T06:01:52.767834Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"response = llama(prompt = prompt_template,\n                max_tokens = 3000,\n                temperature = 0.5,\n                top_p = 0.95,\n                repeat_penalty = 1.2,\n                top_k = 150,\n                echo = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:02:05.870877Z","iopub.execute_input":"2024-09-30T06:02:05.871278Z","iopub.status.idle":"2024-09-30T06:06:30.388077Z","shell.execute_reply.started":"2024-09-30T06:02:05.871243Z","shell.execute_reply":"2024-09-30T06:06:30.387010Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\nllama_print_timings:        load time = 24561.28 ms\nllama_print_timings:      sample time =   283.79 ms /   286 runs   (    0.99 ms per token,  1007.79 tokens per second)\nllama_print_timings: prompt eval time = 24561.18 ms /    38 tokens (  646.35 ms per token,     1.55 tokens per second)\nllama_print_timings:        eval time = 238551.49 ms /   285 runs   (  837.02 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time = 264510.25 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:07:22.268528Z","iopub.execute_input":"2024-09-30T06:07:22.269203Z","iopub.status.idle":"2024-09-30T06:07:22.274125Z","shell.execute_reply.started":"2024-09-30T06:07:22.269163Z","shell.execute_reply":"2024-09-30T06:07:22.273130Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"{'id': 'cmpl-22a4bc4a-cd12-452e-a1d8-62c4a96c57a6', 'object': 'text_completion', 'created': 1727676125, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': \"SYSTEM: You are helpful, respectfull and honest assistant. Always answer as helpfull.\\n\\nUSER: Write a linear regression in python\\n\\nASSISTANT:\\n\\nTo write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\\n```\\nfrom sklearn.linear_model import LinearRegression\\nimport pandas as pd\\n\\n# Load your dataset into a Pandas DataFrame\\ndf = pd.read_csv('your_data.csv')\\n\\n# Create a linear regression object and fit the data\\nreg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\\n\\n# Print the coefficients of the linear regression\\nprint(reg.coef_)\\n```\\nThis will output the coefficients of the linear regression, which you can use to make predictions on new data. For example:\\n```\\n# Make a prediction for a new set of values\\nnew_data = pd.DataFrame({'x1': [2, 3], 'x2': [4, 5]})\\nprediction = reg.predict(new_data)\\nprint(prediction)\\n```\\nThis will output the predicted value for the new data.\\n\\nPlease note that this is just a simple example and there are many other options and parameters you can use when fitting a linear regression model, such as regularization, feature scaling, and more. For more information, please refer to the scikit-learn documentation.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 285, 'total_tokens': 323}}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(response['choices'][0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:07:54.395621Z","iopub.execute_input":"2024-09-30T06:07:54.395996Z","iopub.status.idle":"2024-09-30T06:07:54.401230Z","shell.execute_reply.started":"2024-09-30T06:07:54.395965Z","shell.execute_reply":"2024-09-30T06:07:54.400259Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"SYSTEM: You are helpful, respectfull and honest assistant. Always answer as helpfull.\n\nUSER: Write a linear regression in python\n\nASSISTANT:\n\nTo write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\n```\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\n# Load your dataset into a Pandas DataFrame\ndf = pd.read_csv('your_data.csv')\n\n# Create a linear regression object and fit the data\nreg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\n\n# Print the coefficients of the linear regression\nprint(reg.coef_)\n```\nThis will output the coefficients of the linear regression, which you can use to make predictions on new data. For example:\n```\n# Make a prediction for a new set of values\nnew_data = pd.DataFrame({'x1': [2, 3], 'x2': [4, 5]})\nprediction = reg.predict(new_data)\nprint(prediction)\n```\nThis will output the predicted value for the new data.\n\nPlease note that this is just a simple example and there are many other options and parameters you can use when fitting a linear regression model, such as regularization, feature scaling, and more. For more information, please refer to the scikit-learn documentation.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = 'what is llm and how to fine tune it?'\nprompt_template = f'''SYSTEM: You are helpful, respectfull and honest assistant. Always answer as helpfull.\n\nUSER: {prompt}\n\nASSISTANT:\n'''","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:12:33.899153Z","iopub.execute_input":"2024-09-30T06:12:33.899527Z","iopub.status.idle":"2024-09-30T06:12:33.904044Z","shell.execute_reply.started":"2024-09-30T06:12:33.899492Z","shell.execute_reply":"2024-09-30T06:12:33.903005Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"response = llama(prompt = prompt_template,\n                max_tokens = 3000,\n                temperature = 0.5,\n                top_p = 0.95,\n                repeat_penalty = 1.2,\n                top_k = 150,\n                echo = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:12:36.177101Z","iopub.execute_input":"2024-09-30T06:12:36.177532Z","iopub.status.idle":"2024-09-30T06:17:23.313063Z","shell.execute_reply.started":"2024-09-30T06:12:36.177478Z","shell.execute_reply":"2024-09-30T06:17:23.311812Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time = 27282.87 ms\nllama_print_timings:      sample time =   331.72 ms /   331 runs   (    1.00 ms per token,   997.84 tokens per second)\nllama_print_timings: prompt eval time =  7269.78 ms /    11 tokens (  660.89 ms per token,     1.51 tokens per second)\nllama_print_timings:        eval time = 278200.27 ms /   330 runs   (  843.03 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time = 287127.87 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"print(response['choices'][0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:17:43.368888Z","iopub.execute_input":"2024-09-30T06:17:43.369523Z","iopub.status.idle":"2024-09-30T06:17:43.374492Z","shell.execute_reply.started":"2024-09-30T06:17:43.369481Z","shell.execute_reply":"2024-09-30T06:17:43.373517Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"SYSTEM: You are helpful, respectfull and honest assistant. Always answer as helpfull.\n\nUSER: what is llm and how to fine tune it?\n\nASSISTANT:\nLLM stands for Language Model, which is a type of artificial intelligence (AI) model that can be trained on large amounts of text data to generate language outputs that are coherent and natural-sounding. Fine-tuning an LLM involves adjusting its parameters to improve its performance on a specific task or domain.\n\nThere are several ways to fine-tune an LLM, including:\n\n1. Task-specific tuning: This involves training the model on a specific task, such as sentiment analysis or question answering, and adjusting its hyperparameters to optimize its performance for that task.\n2. Domain adaptation: This involves adapting the model to a new domain or dataset by adding additional data from the target domain during fine-tuning.\n3. Transfer learning: This involves using a pre-trained LLM as a starting point and fine-tuning it on a specific task or domain, leveraging the knowledge learned from the pre-training process.\n4. Hyperparameter tuning: This involves adjusting the model's hyperparameters, such as learning rate, batch size, and number of epochs, to optimize its performance for a specific task or dataset.\n5. Prompt engineering: This involves designing and fine-tuning the input prompts used to elicit responses from the LLM, in order to improve the quality and relevance of the generated text.\n\nThe best approach will depend on the specific use case and goals of the project. It's important to experiment with different techniques and evaluate their effectiveness using appropriate metrics and evaluation methods.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}